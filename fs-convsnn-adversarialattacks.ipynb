{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport math\nimport random\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\n\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom cleverhans.torch.attacks.fast_gradient_method import fast_gradient_method\nfrom cleverhans.torch.attacks.projected_gradient_descent import projected_gradient_descent\n\n\nuse_cuda = True\ndevice = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\nseed = 42\n\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T20:47:28.529483Z","iopub.execute_input":"2023-07-28T20:47:28.530170Z","iopub.status.idle":"2023-07-28T20:47:32.989478Z","shell.execute_reply.started":"2023-07-28T20:47:28.530134Z","shell.execute_reply":"2023-07-28T20:47:32.988620Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Network_ANN(nn.Module):\n    def __init__(self):\n        super(Network_ANN, self).__init__()\n        self.linear_dim = 8192\n        \n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1, bias=False)\n        self.HalfRect1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.subsample1 = nn.MaxPool2d(2, 2, 0)\n        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.HalfRect2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.subsample2 = nn.MaxPool2d(2, 2, 0)\n        self.fc1 = nn.Linear(self.linear_dim, 10, bias=False)\n        self.HalfRect3 = nn.ReLU()\n\n    def to(self, device):\n        self.device = device\n        super().to(device)\n        return self\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.HalfRect1(x)\n        x = self.dropout1(x)\n        x = self.subsample1(x)\n        x = self.conv2(x)\n        x = self.HalfRect2(x)\n        x = self.dropout2(x)\n        x = self.subsample2(x)\n        x = x.view(-1, self.linear_dim)\n        x = self.fc1(x)\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:04:06.195356Z","iopub.execute_input":"2023-07-28T22:04:06.195764Z","iopub.status.idle":"2023-07-28T22:04:06.207961Z","shell.execute_reply.started":"2023-07-28T22:04:06.195728Z","shell.execute_reply":"2023-07-28T22:04:06.206844Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class Surrogate_BP_Function(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input):\n        ctx.save_for_backward(input)\n        out = torch.zeros_like(input).cuda()\n        out[input > 0] = 1.0\n        return out\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad = grad_input * 0.3 * F.threshold(1.0 - torch.abs(input), 0, 0)\n        return grad\n\n\ndef PoissonGen(inp, rescale_fac=2.0):\n    rand_inp = torch.rand_like(inp).cuda()\n    return torch.mul(torch.le(rand_inp * rescale_fac, torch.abs(inp)).float(), torch.sign(inp))\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:04:06.579752Z","iopub.execute_input":"2023-07-28T22:04:06.580496Z","iopub.status.idle":"2023-07-28T22:04:06.589367Z","shell.execute_reply.started":"2023-07-28T22:04:06.580455Z","shell.execute_reply":"2023-07-28T22:04:06.588221Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class Network_SNN(nn.Module):\n    def __init__(self, time_window=30, max_rate=200, threshold=1.0, leak_factor=1.0) -> None:\n        super(Network_SNN, self).__init__()\n        \n        self.image_size = 64\n        self.linear_dim = 8192\n        \n        self.leak_factor = leak_factor\n        self.threshold = threshold        \n        self.spike_fn = Surrogate_BP_Function.apply\n        \n        self.time_window = time_window\n        self.dt = 0.001\n        self.max_rate = max_rate\n        self.rescale_factor = 1.0/(self.dt*self.max_rate)\n        \n        self.conv1 = nn.Conv2d(in_channels=1,out_channels=16,kernel_size=3,stride=1,padding=1,bias=False)\n        self.HalfRect1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.5)\n        self.subsample1 = nn.MaxPool2d(2, 2, 0)\n        self.conv2 = nn.Conv2d(in_channels=16,out_channels=32,kernel_size=3,stride=1,padding=1,bias=False)\n        self.HalfRect2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.5)\n        self.subsample2 = nn.MaxPool2d(2, 2, 0)\n        self.fc1 = nn.Linear(self.linear_dim, 10, bias=False)\n        self.HalfRect3 = nn.ReLU()\n        \n        self.conv_list = [self.conv1, self.conv2]\n        self.pool_list = [self.subsample1, self.subsample2]\n        \n        for m in self.modules():\n            if (isinstance(m, nn.Conv2d)):\n                m.threshold = self.threshold\n            elif (isinstance(m, nn.Linear)):\n                m.threshold = self.threshold\n\n    def to(self, device):\n        self.device = device\n        super().to(device)\n        return self\n    \n    def forward(self, input):\n        infer, _input = input\n        batch_size = _input.size(0)\n            \n        input_spksum = torch.zeros(batch_size, self.conv1.in_channels, self.image_size, self.image_size).to(device)\n        mem_conv1 = torch.zeros(batch_size, self.conv1.out_channels, 64, 64).to(device)\n        mem_conv2 = torch.zeros(batch_size, self.conv2.out_channels, 32, 32).to(device)\n        mem_fc1 = torch.zeros(batch_size, self.fc1.out_features).to(device)\n        mem_conv_list = [mem_conv1, mem_conv2]\n        \n        for t in range(self.time_window):\n            \n            if infer == 'Synth':\n                _t = _input[:,t].view(batch_size, 1, self.image_size, self.image_size)\n            else:\n                _t = _input.view(batch_size, 1, self.image_size, self.image_size)\n                \n            spike_input = PoissonGen(_t.to(device), self.rescale_factor)\n            input_spksum += spike_input\n            out_prev = spike_input\n            \n            for idx in range(len(self.conv_list)):                \n                mem_conv_list[idx] = self.leak_factor * mem_conv_list[idx] + self.conv_list[idx](out_prev)\n                mem_thr = (mem_conv_list[idx] / self.conv_list[idx].threshold) - 1.0\n                out = self.spike_fn(mem_thr)\n                rst = torch.zeros_like(mem_conv_list[idx]).to(device)\n                rst[mem_thr > 0] = self.conv_list[idx].threshold\n                mem_conv_list[idx] = mem_conv_list[idx] - rst\n                out_prev = out.clone()\n                \n                out_prev = self.pool_list[idx](out_prev)\n            \n            out_prev = out_prev.reshape(batch_size, -1)\n            \n            mem_fc1 = mem_fc1 + self.fc1(out_prev)\n\n        out_voltage = mem_fc1 / self.time_window\n\n        return input_spksum, out_voltage\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:04:07.001669Z","iopub.execute_input":"2023-07-28T22:04:07.002034Z","iopub.status.idle":"2023-07-28T22:04:07.022760Z","shell.execute_reply.started":"2023-07-28T22:04:07.002002Z","shell.execute_reply":"2023-07-28T22:04:07.021661Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def OrdinEval(model, params, train_loader=None, test_loader=None, arc='ANN'):\n    model.eval()\n    acc_record = list([])\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for split, loader in [('Train', train_loader), ('Test', test_loader)]:\n            if loader is not None:\n                for batch_idx, (inputs, targets) in enumerate(loader):\n                    batch_sz = inputs.size(0)            \n                    inputs = inputs.float().to(device)\n                    labels_ = torch.zeros(batch_sz, 10).scatter_(1, targets.view(-1, 1), 1).to(device)\n\n                    if arc == 'SNN':\n                        _, outputs = model((params['infer_type'], inputs))\n                    else:\n                        outputs = model(inputs)\n                    targets = targets.to(device)\n\n                    loss = params['criterion'](outputs, labels_)\n                    _, predicted = outputs.max(1)\n                    total += float(targets.size(0))\n                    correct += float(predicted.eq(targets).sum().cpu().item())\n                print(arc, split, \"Acc: %.3f\" % (100 * correct / total), end=' | ')\n                acc = 100. * float(correct) / float(total)\n                acc_record.append(acc)\n\n\ndef OrdinTrainNEval(model, params, train_loader, test_loader, arc='ANN', verbose_interval=4):\n    print(model, '\\nParameter Count:', sum(p.numel() for p in model.parameters() if p.requires_grad),\n          '\\n\\n','**********',arc,'Training **********')\n    verbose_at = [int(len(train_loader)*i/verbose_interval) for i in range(1, verbose_interval+1)]\n\n    for epoch in range(params['num_epochs']):\n        model.train()\n        running_loss = 0\n        start_time = time.time()\n\n        for i, (inputs, targets) in enumerate(train_loader):\n            batch_sz = inputs.size(0)\n            inputs = inputs.float().to(device)\n            labels_ = torch.zeros(batch_sz, 10).scatter_(1, targets.view(-1, 1), 1).to(device)\n\n            params['optimizer'].zero_grad()\n            if arc == 'SNN':\n                _, outputs = model((params['infer_type'], inputs))\n            else:\n                outputs = model(inputs)\n                \n            loss = params['criterion'](outputs, labels_)\n            running_loss += loss.cpu().item()\n            loss.backward()\n            params['optimizer'].step()\n            if i+1 in verbose_at:\n                print ('\\nEpoch [%d/%d], Step [%d/%d], Training Loss: %.5f Time elasped:%.2f s'\n                      %(epoch+1, params['num_epochs'], i+1, len(train_loader),running_loss,time.time()-start_time), end=' | ')\n                running_loss = 0\n        OrdinEval(model, params, train_loader, test_loader, arc)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T20:56:31.265190Z","iopub.execute_input":"2023-07-28T20:56:31.265618Z","iopub.status.idle":"2023-07-28T20:56:31.290559Z","shell.execute_reply.started":"2023-07-28T20:56:31.265567Z","shell.execute_reply":"2023-07-28T20:56:31.289620Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def PerturbedOrdinEval(substitute, target, target_params, dataloader, arc='ANN',\n    attack_info={'epsilon' : 0.3, 'alpha' : 0.01, 'iterations' : 40}) -> None:\n    acc_hist = {}\n    \n    eps = attack_info['epsilon']\n    alpha = attack_info['alpha']\n    iters = attack_info['iterations']\n\n    target_params['infer_type'] = 'Ordin'\n    for adv_func in ['fast_gradient_method', 'projected_gradient_descent']:\n        target.eval()\n        correct = 0\n        total = 0\n\n        for batch_idx, (inputs, targets) in enumerate(dataloader):\n            batch_sz = inputs.size(0)            \n            inputs = inputs.float().to(device)\n            if adv_func == 'fast_gradient_method':\n                perturbed_inputs = fast_gradient_method(substitute, inputs, eps, np.inf)\n            else:\n                perturbed_inputs = projected_gradient_descent(substitute, inputs, eps, alpha, iters, np.inf)\n            labels_ = torch.zeros(batch_sz, 10).scatter_(1, targets.view(-1, 1), 1).to(device)\n\n            if arc == 'SNN':\n                _, outputs = target((target_params['infer_type'], perturbed_inputs))\n            else:\n                outputs = target(perturbed_inputs)\n            targets = targets.to(device)\n\n            loss = target_params['criterion'](outputs, labels_)\n            _, predicted = outputs.max(1)\n            total += float(targets.size(0))\n            correct += float(predicted.eq(targets).sum().cpu().item())\n\n        print(arc, '('+adv_func+')','Perturbed Data', \"Accuracy: %.3f\" % (100 * correct / total))\n        acc = 100. * float(correct) / float(total)\n        acc_hist[adv_func] = acc\n    \n    return acc_hist\n\n\ndef AttackNetworks(substitute, target, target_params, epsilon_range=np.arange(0, 0.35, 0.05), step_size=0.01, iters=40, arc='ANN'):\n    acc_hist = {'fast_gradient_method' : [], 'projected_gradient_descent' : []}\n\n    for eps in epsilon_range:\n        print('Perturbation Strength:', eps)\n        inter_hist = PerturbedOrdinEval(substitute, target, target_params, mnist_test_loader,\n                           attack_info={'epsilon' : eps, 'alpha' : step_size, 'iterations' : iters}, arc=arc); print()\n        for key in inter_hist:\n            acc_hist[key].append(inter_hist[key])    \n    print(acc_hist)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-28T20:56:31.231853Z","iopub.execute_input":"2023-07-28T20:56:31.232239Z","iopub.status.idle":"2023-07-28T20:56:31.247325Z","shell.execute_reply.started":"2023-07-28T20:56:31.232202Z","shell.execute_reply":"2023-07-28T20:56:31.246371Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def mnist_transform(img_size, default=28):\n    pad_xy = (img_size - default) // 2\n    return transforms.Compose([\n        transforms.Pad((pad_xy, pad_xy)),\n        transforms.ToTensor()\n    ])\n\n\nimg_size = 64\ntrain_batch_size = 100\ntest_batch_size = train_batch_size * 2\n\nmnist_train_dataset = torchvision.datasets.MNIST(root=\"D:\\Dataset\\mnist\", train=True, download=True, transform=mnist_transform(img_size))\nmnist_train_loader = torch.utils.data.DataLoader(mnist_train_dataset, batch_size=train_batch_size, shuffle=True)\n\nmnist_test_dataset = torchvision.datasets.MNIST(root=\"D:\\Dataset\\mnist\", train=False, download=True, transform=mnist_transform(img_size))\nmnist_test_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=test_batch_size, shuffle=False)\n\n# import shutil\n# shutil.rmtree('/kaggle/working/D:\\Dataset\\mnist')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T20:48:15.359982Z","iopub.execute_input":"2023-07-28T20:48:15.360352Z","iopub.status.idle":"2023-07-28T20:48:20.714102Z","shell.execute_reply.started":"2023-07-28T20:48:15.360319Z","shell.execute_reply":"2023-07-28T20:48:20.713063Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 376516556.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting D:\\Dataset\\mnist/MNIST/raw/train-images-idx3-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 38455775.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting D:\\Dataset\\mnist/MNIST/raw/train-labels-idx1-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 226564828.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting D:\\Dataset\\mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 4542/4542 [00:00<00:00, 19479068.27it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting D:\\Dataset\\mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to D:\\Dataset\\mnist/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"Substitute_ANN = Network_ANN()\nSubstitute_ANN.to(device)\n\nSubstitute_ANN_Params = {'num_epochs' : 8, 'optimizer' : optim.Adam(Substitute_ANN.parameters(), lr=1e-3),\n    'criterion' : nn.CrossEntropyLoss(reduction='mean').to(device), 'best_acc' : 0}\n\n\nOrdinTrainNEval(Substitute_ANN, Substitute_ANN_Params, mnist_train_loader, mnist_test_loader, verbose_interval=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:04:15.339408Z","iopub.execute_input":"2023-07-28T22:04:15.339781Z","iopub.status.idle":"2023-07-28T22:07:20.763244Z","shell.execute_reply.started":"2023-07-28T22:04:15.339745Z","shell.execute_reply":"2023-07-28T22:07:20.762253Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Network_ANN(\n  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (HalfRect1): ReLU()\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (subsample1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (HalfRect2): ReLU()\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (subsample2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=8192, out_features=10, bias=False)\n  (HalfRect3): ReLU()\n) \nParameter Count: 86672 \n\n ********** ANN Training **********\n\nEpoch [1/8], Step [600/600], Training Loss: 161.12518 Time elasped:11.44 s | ANN Train Acc: 97.003 | ANN Test Acc: 97.029 | \nEpoch [2/8], Step [600/600], Training Loss: 61.36725 Time elasped:11.95 s | ANN Train Acc: 98.102 | ANN Test Acc: 98.104 | \nEpoch [3/8], Step [600/600], Training Loss: 48.44498 Time elasped:10.95 s | ANN Train Acc: 98.348 | ANN Test Acc: 98.337 | \nEpoch [4/8], Step [600/600], Training Loss: 44.07018 Time elasped:11.29 s | ANN Train Acc: 98.653 | ANN Test Acc: 98.627 | \nEpoch [5/8], Step [600/600], Training Loss: 39.96881 Time elasped:11.36 s | ANN Train Acc: 98.827 | ANN Test Acc: 98.804 | \nEpoch [6/8], Step [600/600], Training Loss: 36.40582 Time elasped:10.86 s | ANN Train Acc: 98.618 | ANN Test Acc: 98.581 | \nEpoch [7/8], Step [600/600], Training Loss: 33.51117 Time elasped:11.04 s | ANN Train Acc: 98.877 | ANN Test Acc: 98.849 | \nEpoch [8/8], Step [600/600], Training Loss: 31.71793 Time elasped:11.53 s | ANN Train Acc: 98.995 | ANN Test Acc: 98.957 | ","output_type":"stream"}]},{"cell_type":"code","source":"time_window = 30\nmax_rate = 800\nnum_epochs = 2\n\n\nSNN = Network_SNN(time_window=time_window, max_rate=max_rate)\nSNN.to(device)\n\nSNN_Params = {\n    'num_epochs' : num_epochs, 'optimizer' : optim.Adam(SNN.parameters()), 'criterion' : nn.MSELoss().to(device),\n    'infer_type' : 'Ordin', 'best_acc' : 0\n}\n\n\nOrdinTrainNEval(SNN, SNN_Params, mnist_train_loader, mnist_test_loader, arc='SNN', verbose_interval=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:07:20.765186Z","iopub.execute_input":"2023-07-28T22:07:20.765735Z","iopub.status.idle":"2023-07-28T22:14:06.110461Z","shell.execute_reply.started":"2023-07-28T22:07:20.765696Z","shell.execute_reply":"2023-07-28T22:14:06.109432Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Network_SNN(\n  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (HalfRect1): ReLU()\n  (dropout1): Dropout(p=0.5, inplace=False)\n  (subsample1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (HalfRect2): ReLU()\n  (dropout2): Dropout(p=0.5, inplace=False)\n  (subsample2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (fc1): Linear(in_features=8192, out_features=10, bias=False)\n  (HalfRect3): ReLU()\n) \nParameter Count: 86672 \n\n ********** SNN Training **********\n\nEpoch [1/2], Step [600/600], Training Loss: 12.35645 Time elasped:131.16 s | SNN Train Acc: 97.548 | SNN Test Acc: 97.549 | \nEpoch [2/2], Step [600/600], Training Loss: 7.83382 Time elasped:131.17 s | SNN Train Acc: 98.048 | SNN Test Acc: 98.056 | ","output_type":"stream"}]},{"cell_type":"code","source":"epsilon_range = np.arange(0.15, 0.2, 0.1)\n\nAttackNetworks(Substitute_ANN, SNN, SNN_Params, epsilon_range, arc='SNN')","metadata":{"execution":{"iopub.status.busy":"2023-07-28T22:14:06.112452Z","iopub.execute_input":"2023-07-28T22:14:06.113086Z","iopub.status.idle":"2023-07-28T22:14:45.429833Z","shell.execute_reply.started":"2023-07-28T22:14:06.113048Z","shell.execute_reply":"2023-07-28T22:14:45.428805Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Perturbation Strength: 0.15\nSNN (fast_gradient_method) Perturbed Data Accuracy: 86.740\nSNN (projected_gradient_descent) Perturbed Data Accuracy: 45.600\n\n{'fast_gradient_method': [86.74], 'projected_gradient_descent': [45.6]}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}